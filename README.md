
# Enhancing Subjectivity Detection with Transformer-Based Models: A Case Study with RoBERTa on English News Articles

This repository contains the code and data for the paper "Enhancing Subjectivity Detection with Transformer-Based Models: A Case Study with RoBERTa on English News Articles" by Ahmad Abbas.

## Abstract

The distinction between subjective and objective terms is made in this paperâ€™s discussion of subjectivity categorization in multilingual news items. Our strategy achieves classification by preprocessing and evaluation utilizing F1-macro, Precision, Recall, and F1 scores.

## Requirements

- Python 3.6 or higher
- PyTorch 1.7.0 or higher
- Transformers 4.0.0 or higher
- Pandas 1.1.0 or higher
- Scikit-learn 0.23.2 or higher

## Results

Our results show that RoBERTa outperforms conventional models across languages, providing superior F1 scores for both subjective and objective classes.

## Citation

If you use this code or dataset in your research, please cite our paper:

```
@article{abbas2021enhancing,
  title={Enhancing Subjectivity Detection with Transformer-Based Models: A Case Study with RoBERTa on English News Articles},
  author={Abbas, Ahmad and Mutan, Huthayfa},
  journal={Proceedings of the International Conference on Natural Language Processing and Machine Learning},
  year={2021}
}
```
